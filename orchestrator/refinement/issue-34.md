# Refinement: Issue #34 - [FEATURE] SonarQube MCP Server Integration

**Status**: Refinement Complete
**Generated**: 2026-01-10 09:05:23 UTC

## Clarified Story

Integrate a SonarQube MCP Server as a sibling Docker container managed by McpClientManager so the Orchestrator can query SonarQube metrics during SDLC workflows. OrchestratorConfig will read SONAR_HOST_URL and SONAR_TOKEN from the environment, mask SONAR_TOKEN in logs and never persist it in plaintext. McpClientManager will launch a configurable Sonar MCP container (configurable image from env var), pass proxy variables by default, validate SONAR_CA_BUNDLE_PATH readability if provided, inject SONAR_TOKEN only via container environment variables or container secrets, and complete an MCP initialize/handshake that includes sonarqube_get_new_issues and sonarqube_get_quality_gate_status. Implement PerToolPolicies with resolution order (Per-tool -> operation-type -> global default) supporting full-jitter exponential backoff and allowing PerCallTimeoutSeconds overrides up to 300s. Implement an IProjectMappingService with precedence and TTL caching plus webhook-triggered invalidation. Update CodeReviewExecutor to detect Sonar analysis availability, call sonarqube_get_new_issues when available, and include findings labeled exactly 'SonarQube findings'. Update DodExecutor to call sonarqube_get_quality_gate_status, fail DoD for statuses in a configurable fail-list (default ERROR and WARN), and enforce metric thresholds (default coverage 80%). Conform to .NET 8, Clean Architecture, Repository Pattern, dependency injection (no singletons), avoid Newtonsoft.Json, and cover logic with xUnit unit and integration tests.

## Acceptance Criteria (14)

- Given SONAR_HOST_URL and SONAR_TOKEN are set in the process environment, when OrchestratorConfig initializes, then the system must read SONAR_HOST_URL and SONAR_TOKEN from the environment and must not log SONAR_TOKEN in plaintext during startup or runtime.
- Given SONAR_TOKEN exists, when the system emits logs or persists configuration, then the system must ensure SONAR_TOKEN is never written to logs, persisted in plaintext config files or repos, or returned in any MCP tool responses.
- Given McpClientManager has a configured Sonar MCP image, when McpClientManager initializes, then it must launch a Docker container using the configured image name/tag, default the container Docker NetworkMode to 'bridge' if not overridden, and complete an MCP handshake that lists at minimum sonarqube_get_new_issues and sonarqube_get_quality_gate_status.
- Given the MCP handshake exposes sonarqube_get_new_issues, when CodeReviewExecutor inspects a PR/branch and the Sonar MCP tool returns findings, then CodeReviewExecutor must invoke sonarqube_get_new_issues, parse the response, and include the returned findings in the review output labeled exactly 'SonarQube findings' distinct from any AI-generated findings.
- Given the MCP handshake exposes sonarqube_get_new_issues, when CodeReviewExecutor inspects a PR/branch where Sonar analysis is not available or the tool returns a not-found/no-analysis response, then CodeReviewExecutor must report that Sonar data is unavailable in the review output and must not silently ignore the absence.
- Given DodExecutor runs for a PR/branch, when it invokes sonarqube_get_quality_gate_status and the returned status is included in the configured fail-list (default includes ERROR and WARN), then DodExecutor must fail the DoD, surface the quality gate status and reason in the failure report, and record the failing tool invocation in the operation log without logging secrets.
- Given DodExecutor runs for a PR/branch and sonarqube_get_quality_gate_status returns passing, when reported metrics (for example: code coverage) are below configured thresholds, then DodExecutor must enforce metric thresholds from OrchestratorConfig (default coverage 80%) and must fail the DoD if thresholds are not met, describing which metric failed and its measured value.
- Given a Sonar host URL uses https, when the Sonar MCP container initiates TLS connections, then TLS certificate validation must be enabled by default (SONAR_TLS_VERIFY=true) and McpClientManager must validate that a provided SONAR_CA_BUNDLE_PATH is readable before launching the container.
- Given the Orchestrator process has proxy environment variables present, when McpClientManager starts the Sonar MCP container, then by default the container must receive HTTP_PROXY, HTTPS_PROXY, NO_PROXY and lowercase variants unless SONAR_MCP_PASS_PROXY=false or explicit SONAR_HTTP_PROXY/SONAR_HTTPS_PROXY/SONAR_NO_PROXY overrides are provided.
- Given SONAR_TOKEN is delivered into the MCP container, when McpClientManager starts the container, then SONAR_TOKEN must be injected only via container environment variables or a container secrets mechanism and must not be written to disk in plaintext on the host or persisted by Orchestrator in plaintext.
- Given MCP handshake or tool-invocation network/TLS failures occur, when such failures happen, then McpClientManager and tool-invocation code must surface a clear error, CodeReviewExecutor and DodExecutor must treat Sonar data as unavailable, and the system must attempt retries/timeouts according to the configured resolution order (Per-tool -> operation-type -> global default).
- Given PerToolPolicies permit long-running Sonar calls, when such calls are required, then PerToolPolicies must be able to override PerCallTimeoutSeconds up to 300s, must allow configuration of MaxRetryAttempts, and retries must use full-jitter exponential backoff.
- Given the codebase is instrumented for tests, when unit tests run, then xUnit unit tests must cover: reading and masking of SONAR_HOST_URL and SONAR_TOKEN, McpClientManager container startup logic (mocked), MCP handshake parsing of tool list and contract version, CodeReviewExecutor behavior when Sonar data is present and absent, and DodExecutor behavior for pass/warn/fail statuses and metric threshold enforcement.
- Given an integration test environment with either a running SonarQube instance or a compliant test MCP server, when integration tests are executed, then end-to-end tests must validate: container startup using the configured image, successful MCP handshake exposing Sonar tools, invocation of sonarqube_get_new_issues and sonarqube_get_quality_gate_status, and correct handling of returned results by CodeReviewExecutor and DodExecutor.

## Questions

**How to answer:**
1. Edit this file and add your answer after the question
2. Mark the checkbox with [x] when answered
3. Commit and push changes
4. Remove `blocked` label and add `dor` label to re-trigger

- [x] **Question #2:** What level of integration test coverage is required for CI: are mocked MCP server smoke tests acceptable in CI pipelines, or must CI include full scenario tests that run against a real Sonar instance and sample project analysis? If real-instance tests are required, will a hosted SonarCloud account/test project be provided or must a local Sonar instance be run in CI?
  **Answer (ProductOwner):** Per the provided requirements and acceptance criteria, CI must always run unit tests and fast integration smoke tests that use a compliant/mocked MCP server (or a lightweight test MCP) to validate container startup, MCP handshake, and basic sonarqube_* tool invocation behavior. Full end-to-end scenario tests against a real SonarQube instance (or a full Sonar-compliant test MCP server that performs real analysis) are required as part of the integration test suite, but they do not have to run on every PR build; they should run in a dedicated integration pipeline (e.g., nightly, gated merge, or release pipeline) because they are longer running, require credentials, and are more brittle. The acceptance criteria explicitly allow integration tests to run against either a running SonarQube instance or a compliant test MCP server, so the team can choose either approach for the full scenario pipeline. Whether a hosted SonarCloud account/test project will be provided is not specified in the provided context.

- [x] **Question #3:** Are there mandatory compliance or secrets-management requirements for SONAR_TOKEN storage and rotation we must follow or document (for example: required use of the organization's secrets manager, rotation cadence, audit logging)? If yes, please point to the policy or required mechanism.
  **Answer (ProductOwner):** The provided requirements and acceptance criteria do not mandate any organizational compliance policy (e.g., specific secrets manager, rotation cadence, or audit logging mechanism). They DO mandate technical constraints we must implement: SONAR_TOKEN must be read from the environment by OrchestratorConfig, must never be logged in plaintext or persisted in plaintext config files or repos, must not be returned in MCP tool responses, and when delivered to the Sonar MCP container must be injected only via container environment variables or a container secrets mechanism and never written to disk on the host. Given the absence of an organizational policy in the spec, you should document and adopt a formal secrets-management practice. Recommended minimum requirements to include in documentation and implementation guidance are: 1) Use an approved secrets manager (e.g., HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, or the org-approved secret-store) to store SONAR_TOKEN and provide short-lived credentials to runtimes where possible; 2) Inject secrets into the Sonar MCP container via the container runtime’s secret mechanism (not host files), or via environment variables populated from the secrets manager at runtime; 3) Do not persist tokens in source control, config files, or application logs; ensure tokens are masked in all logs and telemetry; 4) Establish and document a rotation policy (recommended cadence: at most 90 days, or follow org policy) and a documented process to rotate/revoke tokens used by CI/Orchestrator; 5) Ensure RBAC limits who can read or manage the SONAR_TOKEN and require MFA for administrative actions where supported; 6) Enable and retain audit logs for secrets access (secrets manager and container orchestration actions) per organizational retention policy; 7) Provide an emergency revocation and redeployment procedure for compromised tokens; 8) Add automated tests and CI gates to ensure SONAR_TOKEN is not leaked into artifacts or test logs. These recommendations should be codified in the project security/secrets-management doc and aligned with the organization’s formal policies.

- [x] **Question #4:** What default MCP container image name and tag should be used if no explicit image is provided (the spec currently references sonarsource/sonarqube-mcp-server but image name/tag is TBC)? Should the image default be pinned to a specific tag in OrchestratorConfig or must it always be explicitly supplied?
  **Answer (ProductOwner):** Require the image to be explicitly configurable in OrchestratorConfig and prefer a pinned image reference (ideally an immutable digest) rather than an unpinned implicit default. Concretely: add a nullable config property (e.g. SonarMcpImage) populated from an env var (SONAR_MCP_IMAGE). On startup validate the value. Behavior policy: - Production/CI: fail-fast if SonarMcpImage is missing. - Development/local: allow a documented fallback (only) such as sonarsource/sonarqube-mcp-server:latest or a pinned dev tag, but log a strong warning. Implementation recommendations: store and document a pinned tag or image@sha256 digest as the canonical value used by releases, and update that value in release notes when you bump the MCP image. Never hardcode or silently default to :latest for production; prefer an explicit pinned tag or digest.

## Ambiguous Questions (1)

**These questions require human clarification:**
They mix product and technical concerns and need stakeholder input to determine the correct approach.

**How to clarify:**
1. Add a comment to the GitHub issue with clarifications
2. Remove `blocked` and `user-review-required` labels
3. Add the `dor` label to re-trigger refinement

- [x] **Question #1:** Do we need to support multiple Sonar instances (multiple SONAR_HOST_URL/SONAR_TOKEN pairs)? If yes, how should credentials and instance selection be specified (per-repo config file, multi-entry OrchestratorConfig map, or an external credential store)?
   **Answer (User):** No, it is not needed

