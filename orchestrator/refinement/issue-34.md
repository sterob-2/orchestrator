# Refinement: Issue #34 - [FEATURE] SonarQube MCP Server Integration

**Status**: Refinement Complete
**Generated**: 2026-01-09 20:10:06 UTC

## Clarified Story

Integrate a SonarQube MCP Server as a sibling Docker container managed by McpClientManager so the Orchestrator can query SonarQube metrics during SDLC workflows. OrchestratorConfig must read SONAR_HOST_URL and SONAR_TOKEN from environment variables and must never log or persist SONAR_TOKEN in plaintext; SONAR_TOKEN must only be supplied securely to the MCP container (via environment or secret mechanism). McpClientManager must be able to launch a configurable MCP container image via Docker, perform an MCP handshake that lists available tools (at minimum sonarqube_get_new_issues and sonarqube_get_quality_gate_status), and allow programmatic invocation/parsing of those tools per the MCP contract. Implement IProjectMappingService that composes RepoFileMappingRepository, CentralOverrideRepository, and NamingConventionFallback with resolution precedence (central override -> repo mapping file -> deterministic naming fallback), in-memory TTL caching, and webhook-triggered invalidation. CodeReviewExecutor must detect Sonar analysis presence, invoke sonarqube_get_new_issues when available, and include returned findings labelled 'SonarQube findings' distinct from AI findings; if unavailable it must report Sonar data as unavailable with configurable retry behavior. DodExecutor must invoke sonarqube_get_quality_gate_status, fail DoD when status is in a configured fail-list (default ERROR and WARN), and additionally enforce configured metric thresholds (default coverage threshold = 80%) when gates are insufficient. All handshake and tool invocation errors, network/TLS failures, and token handling must surface clear errors and follow configurable retry/timeouts in OrchestratorConfig. Implementation must follow .NET 8, Clean Architecture, Repository Pattern, dependency injection (no singletons), avoid Newtonsoft.Json, use xUnit for tests, include unit and integration tests, and provide CI documentation for SonarScanner and test steps.

## Acceptance Criteria (10)

- Given SONAR_HOST_URL and SONAR_TOKEN are set in the environment, when OrchestratorConfig is initialized, then the Orchestrator must read SONAR_HOST_URL and SONAR_TOKEN from the environment and must not log these values in plaintext during startup or runtime.
- Given McpClientManager has a configured Sonar MCP image, when McpClientManager initializes, then it must launch a Docker container using the configured image name/tag and perform an MCP handshake that lists Sonar-related tools including at minimum sonarqube_get_new_issues and sonarqube_get_quality_gate_status.
- Given the MCP handshake exposes sonarqube_get_new_issues, when CodeReviewExecutor checks a PR/branch that has Sonar analysis available, then it must invoke sonarqube_get_new_issues, receive a parseable response, and include returned Sonar findings in the review output clearly labeled as 'SonarQube findings' (distinct from AI-generated findings).
- Given the MCP handshake exposes sonarqube_get_new_issues, when CodeReviewExecutor inspects a PR/branch where Sonar analysis is not available or the tool returns a not-found/no-analysis response, then the CodeReviewExecutor must report that Sonar data is unavailable and must not fail silently.
- Given DodExecutor runs for a PR/branch, when it invokes sonarqube_get_quality_gate_status and the returned status is in the configured fail-list (default includes ERROR and WARN), then the DodExecutor must fail the DoD and surface the quality gate status and reason in the failure report.
- Given DodExecutor runs for a PR/branch and sonarqube_get_quality_gate_status returns a passing gate but the reported metrics (for example: code coverage) are below configured thresholds (default coverage threshold = 80%), when thresholds are configured in OrchestratorConfig, then DodExecutor must enforce the configured metric thresholds independently and fail the DoD if thresholds are not met.
- Given SONAR_TOKEN is provided to the system, when the system logs, stores, or returns data, then SONAR_TOKEN must never be written to logs, persisted in plaintext in the repository or config files, or returned in MCP tool responses; the token must only be supplied to the MCP container via environment or other secure mechanism.
- Given network or TLS failures occur during MCP handshake or tool invocation, when such failures happen, then the McpClientManager and tool-invocation code must surface a clear error, CodeReviewExecutor and DodExecutor must treat Sonar data as unavailable, and configured retry behavior must be attempted according to OrchestratorConfig.
- Given the codebase and test suite, when unit tests are run, then unit tests must cover: reading and masking of SONAR_HOST_URL/SONAR_TOKEN, McpClientManager container startup logic (mocked), handshake parsing of tool list and contract version, CodeReviewExecutor behavior when Sonar data is present and absent, and DodExecutor behavior for pass/fail/warn statuses and metric threshold enforcement (xUnit, .NET 8).
- Given an integration test environment with either a running SonarQube instance or a test MCP server, when integration tests are executed (or documented local/integration steps are followed), then the end-to-end flow must be validated: container startup using the configured image, successful handshake exposing Sonar tools, invocation of sonarqube_get_new_issues and sonarqube_get_quality_gate_status, and correct handling of returned results by CodeReviewExecutor and DodExecutor.

## Open Questions (6)

**How to answer:**
1. Add a comment to the GitHub issue with your answers
2. Remove `blocked` and `user-review-required` labels
3. Add the `dor` label to re-trigger refinement

Refinement will read your comment, incorporate answers, and stop re-asking those questions.

- [ ] **Question #1:** What networking mode and TLS/proxy expectations should the Sonar MCP container use when communicating with the Sonar host? Specifically: should the container use bridge or host networking by default, must we support custom CA bundles and explicit TLS certificate validation toggles, and should proxy environment variables/custom CA bundles be passed through/configurable? Please state required defaults and which options must be configurable.
- [ ] **Question #2:** What are the concrete default retry and timeout policies for MCP handshake and tool invocations that we should implement and expose in OrchestratorConfig? Provide defaults: per-call timeout seconds, max retry attempts, backoff strategy (linear/exponential) and base backoff duration, and state whether these are per-tool or global.
- [ ] **Question #3:** Do we need to support multiple Sonar instances (multiple SONAR_HOST_URL/SONAR_TOKEN pairs)? If yes, how should credentials and instance selection be specified (per-repo config file, multi-entry OrchestratorConfig map, or an external credential store)?
- [ ] **Question #4:** What level of integration test coverage is required for CI: are smoke tests against a mocked MCP server acceptable in CI pipelines, or must CI include full scenario tests that run against a real Sonar instance and sample project analysis? If real-instance tests are required, will a hosted SonarCloud account/test project be provided or must a local Sonar instance be run in CI?
- [ ] **Question #5:** Are there any mandatory compliance or secrets-management requirements for SONAR_TOKEN storage and rotation we must follow or document (for example: required use of the organization's secrets manager, rotation cadence, audit logging)? If yes, please point to the policy or required mechanism.
- [ ] **Question #6:** What default MCP container image name/tag should be used if no explicit image is provided (we currently reference sonarsource/sonarqube-mcp-server but image name/tag is TBC)? Should the image default be pinned to a specific tag in OrchestratorConfig or must it always be explicitly supplied?

